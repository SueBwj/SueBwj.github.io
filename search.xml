<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>CNN</title>
      <link href="/2024/05/14/CNN/"/>
      <url>/2024/05/14/CNN/</url>
      
        <content type="html"><![CDATA[<h3 id="Neuron-Version-Story"><a href="#Neuron-Version-Story" class="headerlink" title="Neuron Version Story"></a>Neuron Version Story</h3><ul><li>一张图片是 3 维的 tensor，(channel, height, weight)。channel 表示 rgb 三通道；将图片的三维压缩成一维度向量输入到神经网络<br><img src="/../CNN_img/Untitled.png" alt="Untitled"></li><li>如果使用<strong>Fully Connected Network</strong>，需要的参数量非常巨大，输入向量长度为 100<em>100</em>3， 神经元个数是 1000 个，模型参数越多，越容易 overfiting</li><li>Observation1<ul><li>神经元的作用：Identifying some critical patterns，发现一些重要的特征<br><img src="/../CNN_img/Untitled%201.png" alt="Untitled"></li><li>Some patterns are much smaller than the whole image.特征只占图片的一小部分<ol><li>每一个 neural 只关注一小部分（receptive field）就好了</li><li>receptive field 可以重叠</li><li>receptive field 可以只 cover 部分 channel</li><li>receptive field 不一定要是正方形<br><img src="/../CNN_img/Untitled%202.png" alt="Untitled"><br><img src="/../CNN_img/Untitled%203.png" alt="Untitled"></li></ol></li><li>Typical Setting<ol><li>考虑所有的 channel</li><li>receptive field 也被称为 kernel size，一般设置为 3*3</li><li>同一个 receptive field 会有一组，一排去检测它（64 个神经元）</li><li>stride 表示每个 receptive field 之间的间隔是多少 （stride &#x3D; 2），最好每个 receptive field 之间有一定的重叠</li><li>receptive field 有一部分超出了图片的范围，那么就用 padding 来补充<br><img src="/../CNN_img/Untitled%204.png" alt="Untitled"></li></ol></li></ul></li><li>Observation2:<ul><li>The same patterns appear in different regions<ul><li>每一个 receptive field 都有一个侦测鸟嘴的神经元 → 问题：参数量太多了<br><img src="/../CNN_img/Untitled%205.png" alt="Untitled"></li><li>解决方案：<strong>参数共享</strong>，虽然参数一样，但是输入是不一样的，所以输出是不一样的<br><img src="/../CNN_img/Untitled%206.png" alt="Untitled"></li><li>Typical Setting - 常见的共享参数的方法<br><img src="/../CNN_img/Untitled%207.png" alt="Untitled"></li></ul></li></ul></li></ul><h3 id="Convolution-Layer-—-another-story-based-on-filter"><a href="#Convolution-Layer-—-another-story-based-on-filter" class="headerlink" title="Convolution Layer — another story based on filter"></a>Convolution Layer — another story based on filter</h3><ol><li>从图片中去抓取 pattern</li></ol><ul><li>计算方式<br><img src="/../CNN_img/Untitled%208.png" alt="表明左上角和左下角都出现了这个pattern"><br>表明左上角和左下角都出现了这个 pattern</li><li>future map<br>如果有 64 个 filter，那么就会有 64<em>4</em>4 的矩阵，可以把这个 future map 理解成一张新的图片，有 64 个 channel<br><img src="/../CNN_img/Untitled%209.png" alt="Untitled"></li></ul><ol><li>如果 network 足够深，filter 能够看到足够大的 pattern</li></ol><ul><li><strong>Observation3</strong><ol><li>Subsampling the pixels will not change the object - 放大缩小(减去行和列的条数)图片不会对图像内容造成影响</li></ol><ul><li>Pooling - Max Pooling<ul><li>Pooling 的作用 - 减少运算量，如果运算量足够强，也可以不做 pooling<br><img src="/../CNN_img/Untitled%2010.png" alt="Untitled"></li><li>Convolutional Layers + Pooling<br><img src="/../CNN_img/Untitled%2011.png" alt="Untitled"></li></ul></li></ul></li></ul><h3 id="The-whole-CNN"><a href="#The-whole-CNN" class="headerlink" title="The whole CNN"></a>The whole CNN</h3><p><img src="/../CNN_img/Untitled%2012.png" alt="Untitled"></p><p><img src="/../CNN_img/Untitled%2013.png" alt="Untitled"></p><aside>❓ CNN无法处理放大和缩小之后或旋转的图像，CNN is not invariant to scaling and rotation</aside><p>→ need data augmentation — 处理旋转问题</p><p>→ 处理放大缩小问题 — Spatial Transformer Layer</p><aside>💡 通过影像的特性来对模型进行限制，避免overfitting</aside>]]></content>
      
      
      
        <tags>
            
            <tag> 李宏毅机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Markov Decision Process - 马尔可夫决策过程</title>
      <link href="/2024/05/08/Markov-Decision-Process-%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B/"/>
      <url>/2024/05/08/Markov-Decision-Process-%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<h2 id="defination"><a href="#defination" class="headerlink" title="defination"></a>defination</h2><p>在数学中，<a href="https://zh.wikipedia.org/wiki/%E9%A6%AC%E5%8F%AF%E5%A4%AB%E6%B1%BA%E7%AD%96%E9%81%8E%E7%A8%8B" title="Markdown">马尔可夫决策过程（Markov decision process，MDP）</a>是离散时间随机控制过程。 它提供了一个数学框架，用于在结果<strong>部分随机</strong>且<strong>部分受决策者控制</strong>的情况下对决策建模。</p><p>首先要考虑以下三个条件：state、action、function</p><h3 id="1-states"><a href="#1-states" class="headerlink" title="1. states"></a>1. states</h3><p>考虑状态要从两个方面出发，一个是初始状态的设置，一个是过程中的状态变化。</p><h4 id="Initial-Conditions（初始设置）-："><a href="#Initial-Conditions（初始设置）-：" class="headerlink" title="Initial Conditions（初始设置） ："></a>Initial Conditions（初始设置） ：</h4><p>在设置初始条件的时候要设置模型的<strong>初始状态(start state)<strong>和</strong>终止状态(terminal state)</strong>,要保证模型最终能够达到终止状态，如果无法达到终止状态(the game last forever),有以下两种解决方案：</p><ul><li><p>Terminate episodes after a fixed T steps <strong>(e.g. life)</strong></p></li><li><p>Gives nonstationary policies (p depends on time)</p></li></ul><h4 id="process-过程中-："><a href="#process-过程中-：" class="headerlink" title="process(过程中)："></a>process(过程中)：</h4><p>考虑过程中的状态变化时，要考虑：<strong>当前状态(current_state)<strong>、</strong>下一状态(next_state、s’)</strong> 以及在当前状态和动作已知的情况下的 <strong>Qstate</strong></p><h3 id="2-action"><a href="#2-action" class="headerlink" title="2. action"></a>2. action</h3><p>采取何种行为到达下一状态。例如：在地图中，可以选择上下左右四个动作到达不同的状态</p><h3 id="3-functions"><a href="#3-functions" class="headerlink" title="3. functions"></a>3. functions</h3><ul><li><p>transition function T(s,a,s’)</p><p>转移函数，即当前状态 s 采取 action 后到达下一个状态 s’的概率大小</p></li><li><p>reward functions R(s,a,s’)</p><p>奖励函数，即当前状态 s 采取 action 后到达下一个状态 s’会给予 agent 多少奖励；或者这一步移动会给你带来多大收益</p><ul><li><p>discount</p><p>奖励函数(reward function)可以考虑设置 discount（折扣），毕竟<strong>今天</strong>获得 100 元和<strong>1 年后</strong>获得 100 元的价值对于当前的你来说是不一样的，人们肯定更倾向于现在获得 100 元，所以可以通过 discount 来模拟模型随时间的变化，设置 discount 也是解决上述无法达到终止状态的解决方案</p></li></ul></li></ul><p>当考虑完以上三个条件的设定之后，便可以考虑如何设置策略(policy)和如何衡量当前状态的价值(value of state)</p><h2 id="policies"><a href="#policies" class="headerlink" title="policies"></a>policies</h2><h3 id="defination-1"><a href="#defination-1" class="headerlink" title="defination"></a>defination</h3><ul><li><p>A policy p gives an action for each state – 争对每一个状态都给出应该采取的动作</p></li><li><p>An <strong>optimal policy</strong> is one that maximizes expected utility if followed – 最优的 policy 就是争对每一个状态给出的动作都能使收益达到最大</p></li></ul><h2 id="Value-of-a-state"><a href="#Value-of-a-state" class="headerlink" title="Value of a state"></a>Value of a state</h2><h3 id="defination-2"><a href="#defination-2" class="headerlink" title="defination"></a>defination</h3><p>当前状态的价值即从 start state 到 current state 一直采取最优策略获得的收益，一般用 V*表示 <code>V\*(s) -- expected utility starting in s and acting optimally</code></p><h3 id="用于计算-value-of-state-的一些指标"><a href="#用于计算-value-of-state-的一些指标" class="headerlink" title="用于计算 value of state 的一些指标"></a>用于计算 value of state 的一些指标</h3><ul><li>V*(s)</li></ul><p><img src="/../../public/images/V%E7%9A%84%E8%AE%A1%E7%AE%97%E5%85%AC%E5%BC%8F.png" alt="V*的计算公式"></p><ul><li>Q*(s,a) – expected utility starting out having taken action a from state s and<br>(thereafter) acting optimally</li></ul><p><img src="/../../public/images/Q%E7%9A%84%E8%AE%A1%E7%AE%97%E6%96%B9%E6%B3%95.png" alt="Q*的计算公式"></p><ul><li>pi*(s) – optimal action from state s</li></ul><h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><p>马尔可夫决策的目的就是找到最优的 policy 从 initial state 到达 terminal state，以下介绍两种方法计算出最优的 policy</p><h3 id="Value-Iteration"><a href="#Value-Iteration" class="headerlink" title="Value Iteration"></a>Value Iteration</h3><ul><li><p>pursucode 算法步骤</p><ul><li><ol><li>start with v0(s) &#x3D; 0 初始状态 value 设置为 0</li></ol></li><li><p><img src="/../../public/images/value_iteration.png" alt="value_iteration"></p></li><li><ol start="3"><li>repeat until converge</li></ol></li></ul></li><li><p>complexity</p><ul><li>O(S^2*A)</li></ul></li><li><p>problem</p><ul><li><ol><li>slow</li></ol></li><li><ol start="2"><li>argmax</li></ol></li></ul></li></ul><h3 id="Policy-Iteration"><a href="#Policy-Iteration" class="headerlink" title="Policy Iteration"></a>Policy Iteration</h3><ul><li><p>Method</p><ul><li><p>Policy Evaluation</p><ul><li><p>优点</p><ul><li></li></ul></li><li><p>Bellman equation（用来计算采取当且 policies 的 utilities）</p><ul><li></li></ul></li></ul></li><li><p>Policy Extraction</p><ul><li><p>Computing actions from values</p><ul><li></li></ul></li><li><p>Computing actions from Q-values</p><ul><li></li></ul></li></ul></li></ul></li><li><p>算法</p><ul><li><ol><li>Evaluation</li></ol><ul><li><p>calculate utilities for some fixed policy (not optimal utilities!) until convergence</p><ul><li></li></ul></li></ul></li><li><ol start="2"><li>Improvment</li></ol><ul><li><p>update policy using one-step look-ahead with resulting converged (but not optimal!) utilities as future values</p><ul><li></li></ul></li></ul></li><li><ol start="3"><li>repeat</li></ol></li></ul></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> MDP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>形式与政策</title>
      <link href="/2024/05/05/%E5%BD%A2%E5%BC%8F%E4%B8%8E%E6%94%BF%E7%AD%96/"/>
      <url>/2024/05/05/%E5%BD%A2%E5%BC%8F%E4%B8%8E%E6%94%BF%E7%AD%96/</url>
      
        <content type="html"><![CDATA[<p>💀 一直对学校中形式主义的教学和任务深恶痛绝，为减轻他人负担，故分享 <strong>此文章由 GPT4 生成，可随性引用</strong></p><h1 id="琢磨全球风云：新时代大学生的国际视野与责任担当"><a href="#琢磨全球风云：新时代大学生的国际视野与责任担当" class="headerlink" title="琢磨全球风云：新时代大学生的国际视野与责任担当"></a>琢磨全球风云：新时代大学生的国际视野与责任担当</h1><p>作为新时代大学生，“风声雨声读书声，声声入耳；家事国事天下事，事事关心”是对我们最基本的要求。而《形式与政策》这门课程给了我一个机会去学习，去了解当今社会的发展状况及趋势。社会的大发展已决定了个人发展的最大环境、最大上限，制约着可选择度，决定着大学生成功的机率，影响很具体，也很深远。因此，我们应学会认识和把握形势与政策。形势是制定政策的依据，政策影响形势的发展。如今世界飞速发展，各个国家的形势与政策也变化莫测。作为大学生的我们，岂能做那四角的书柜。新时代的接班人就该有自己的思想，不能人云亦云，需要我们形成对形势与政策的洞察力和深刻的理解力，培养超强的把握形势与政策的胆识。以下我将根据课堂上对国际局势和中国原则立场的阐述谈谈我的学习心得。</p><h4 id="一、动荡变革的世界与中国的角色"><a href="#一、动荡变革的世界与中国的角色" class="headerlink" title="一、动荡变革的世界与中国的角色"></a>一、动荡变革的世界与中国的角色</h4><p>当前国际形势波诡云谲，全球政治经济格局正在经历深刻的变动。课堂中指出，地缘政治的紧张、全球经济的复苏困难以及极端气候事件的增多，都是这一时代的特征。而中国，在这一全球变局中所扮演的角色愈发显得至关重要。作为世界的平稳器和连接者，中国在多极世界中积极推动建设性的国际关系，倡导合作共赢的全球治理观。这种外交策略不仅展示了中国的大国担当，也为世界的和平与发展提供了中国智慧和中国方案。</p><h4 id="二、中美关系的新阶段与个人思考"><a href="#二、中美关系的新阶段与个人思考" class="headerlink" title="二、中美关系的新阶段与个人思考"></a>二、中美关系的新阶段与个人思考</h4><p>探究中美关系的稳定与变化，我感受到了这种宏观关系对个人生活轨迹的深远影响。中美之间的每一次微妙平衡，都可能成为影响我未来职业和学术道路的关键因素。在这不断变化的国际环境中，我深知作为学生的我，更应深入学习国际政治经济知识，为未来不可预测的挑战做好准备。</p><h4 id="三、理解多极世界与塑造自我能力"><a href="#三、理解多极世界与塑造自我能力" class="headerlink" title="三、理解多极世界与塑造自我能力"></a>三、理解多极世界与塑造自我能力</h4><p>在多极世界秩序加速整合的今天，了解和适应这种新的国际关系格局，对我们每个人都是一次挑战也是一次机遇。报告中对当前国际秩序的深刻解析，让我认识到，作为新时代的青年，我们需要有开阔的国际视野，更需要有应对复杂国际局势的能力。面对全球性的挑战，如安全、经济和环境问题，作为国际社会的一员，我们每个人都承担着共同的责任。从职业选择到日常行为，每一步都是对全球责任的践行。如何通过自己的专业知识和技能为全球可持续发展做出贡献，成为我深入思考的问题，也是我作为新时代青年的使命所在。</p><p>总之，通过形式与政策这门课的学习，我更加坚信，作为新时代的青年，我们需要站在历史的高度，以开放的心态和包容的姿态，面对全球化带来的挑战和机遇，积极作为，为构建人类命运共同体贡献自己的青春力量。这种力量，源自对和平的渴望，对发展的追求，也源自对美好未来的共同憧憬。</p>]]></content>
      
      
      
        <tags>
            
            <tag> formalism </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2024/04/29/hello-world/"/>
      <url>/2024/04/29/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
